{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941c0001-b75b-45dc-8d02-881dcd45ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6503362c-e962-47c4-896f-917c13d7e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyspark\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import hour, minute, second, year, month, dayofmonth, dayofweek, count, to_timestamp, when, isnan\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from pyspark.sql.functions import countDistinct, udf\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5475fc-bc74-497f-955a-4717df8b1b93",
   "metadata": {},
   "source": [
    "# Check unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb4b430-d447-4d51-8fe1-6dca94d3d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import hdfs\n",
    "import subprocess\n",
    "\n",
    "def run_cmd(args_list):\n",
    "    \"\"\"\n",
    "    run linux commands\n",
    "    \"\"\"\n",
    "    # import subprocess\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    s_output, s_err = proc.communicate()\n",
    "    s_return =  proc.returncode\n",
    "    return s_return, s_output, s_err \n",
    "\n",
    "def get_s3_hdfs_idff():\n",
    "    # Get file list from s3\n",
    "    session = boto3.session.Session()\n",
    "\n",
    "    s3 = session.client(\n",
    "        service_name='s3',\n",
    "        endpoint_url='https://storage.yandexcloud.net'\n",
    "    )\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket='fraud-data')\n",
    "    s3_files = list(map(lambda item: item.get('Key'), response.get('Contents', [])))\n",
    "    \n",
    "    # Get file list from hdfs\n",
    "    (ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', '/user/fraud-data'])\n",
    "    hdfs_files = [os.path.basename(line.rsplit(None,1)[-1]) for line in out.decode().split('\\n') if len(line.rsplit(None,1))]\n",
    "    \n",
    "    new_files = set(s3_files) - set(hdfs_files)\n",
    "    return new_files\n",
    "\n",
    "def get_new_files_list():\n",
    "    processed_files = []\n",
    "    processed_store_file = 'processed.txt'\n",
    "    if os.path.exists(processed_store_file):\n",
    "        with open('processed.txt', 'r') as pr_file:\n",
    "            processed_files = pr_file.readlines()\n",
    "        \n",
    "    # Get file list from hdfs\n",
    "    (ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', '-C', '/user/fraud-data'])\n",
    "    hdfs_files = [os.path.basename(line) for line in out.decode().split('\\n') if len(os.path.basename(line))]\n",
    "    \n",
    "    new_files = set(hdfs_files).difference(set(processed_files))\n",
    "    new_files = list(map(lambda f: \"/user/fraud-data/\" + f, new_files))\n",
    "    new_files.sort()\n",
    "    return new_files\n",
    "\n",
    "def get_files_list():        \n",
    "    # Get file list from hdfs\n",
    "    (ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', '-C', '/user/fraud-data'])\n",
    "    hdfs_files = [line for line in out.decode().split('\\n') if len(line)]\n",
    "    hdfs_files.sort()\n",
    "    return hdfs_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068ccb3-c317-4306-b145-aa564db17457",
   "metadata": {},
   "source": [
    "# Read and preprocess new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12ea58c-9433-44d9-bb98-9a25220be5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame\n",
    "\n",
    "def read_files(file_list):\n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), True),\n",
    "        StructField(\"tx_datetime\", StringType(), True),\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"terminal_id\", IntegerType(), True),\n",
    "        StructField(\"tx_amount\", DoubleType(), True),\n",
    "        StructField(\"tx_time_seconds\", IntegerType(), True),\n",
    "        StructField(\"tx_time_days\", IntegerType(), True),\n",
    "        StructField(\"tx_fraud\", IntegerType(), True),\n",
    "        StructField(\"tx_fraud_scenario\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = (spark.read\n",
    "        .format(\"csv\")\n",
    "        .schema(schema)\n",
    "        .option(\"header\", False)\n",
    "        .option(\"sep\", ',')\n",
    "        .option(\"comment\", '#')\n",
    "        .load(file_list)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def explore(df):\n",
    "    # Show the DataFrame\n",
    "    df.show()\n",
    "    \n",
    "    row_count = df.count()\n",
    "    print(f\"Row count: {row_count}\")\n",
    "    \n",
    "    # Count the number of NaN values in each column\n",
    "    #print(\"Count of NaN values in each column:\")\n",
    "    #nan_count = df.select([count(when(isnan(c), c)).alias(c) for c in df.columns])\n",
    "    #nan_count.show()\n",
    "    \n",
    "    #df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "    \n",
    "    #for column in [\"customer_id\", \"terminal_id\", \"tx_fraud_scenario\"]:\n",
    "    #    distinct_count = df.agg(countDistinct(column)).collect()[0][0]\n",
    "    #    print(f\"{column}: {distinct_count} unique values\")\n",
    "    \n",
    "    df.groupBy('tx_fraud').count().show()\n",
    "\n",
    "\n",
    "# Define a UDF to handle the special case\n",
    "def convert_timestamp(s):\n",
    "    if s[11:13] == '24':\n",
    "        return s[:11] + '00' + s[13:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    # Convert the tx_datetime column to a timestamp type\n",
    "    df = df.limit(df.count() // 10)\n",
    "    convert_timestamp_udf = udf(convert_timestamp)\n",
    "    df = df.withColumn(\"tx_datetime\", convert_timestamp_udf(df[\"tx_datetime\"]))\n",
    "    df = df.withColumn(\"ts\", to_timestamp(df[\"tx_datetime\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    \n",
    "    df = df.fillna({'terminal_id': 0})\n",
    "    \n",
    "    # Extract new features from the tx_datetime column\n",
    "    df = df.withColumn(\"is_weekend\", dayofweek(\"ts\").isin([1,7]).cast(\"int\"))\n",
    "    df = df.withColumn(\"year\", year(df[\"ts\"]))\n",
    "    df = df.withColumn(\"month\", month(df[\"ts\"]))\n",
    "    df = df.withColumn(\"day_of_month\", dayofmonth(df[\"ts\"]))\n",
    "    df = df.withColumn(\"day_of_week\", dayofweek(df[\"ts\"]))\n",
    "    df = df.withColumn(\"hour\", hour(df[\"ts\"]))\n",
    "    df = df.withColumn(\"minute\", minute(df[\"ts\"]))\n",
    "    df = df.withColumn(\"second\", second(df[\"ts\"]))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def build_train_pipeline():\n",
    "    # Define the pipeline for feature extraction and transformation\n",
    "    # Here, we'll use StringIndexer and OneHotEncoder for categorical features.\n",
    "    # For numerical features, we'll use StandardScaler to scale them.\n",
    "    # Define the pipeline stages\n",
    "    stages = []\n",
    "\n",
    "    # Terminal_id, hour_of_day, and day_of_week should be treated as categorical features\n",
    "    #categorical_columns = [\"hour\", \"day_of_week\", \"month\"]\n",
    "    #for column in categorical_columns:\n",
    "    #    string_indexer = StringIndexer(inputCol=column, outputCol=f\"{column}_index\", handleInvalid=\"keep\")\n",
    "    #    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[f\"{column}_ohe\"])\n",
    "    #    stages += [string_indexer, encoder]\n",
    "\n",
    "    # Define numerical columns and apply StandardScaler\n",
    "    numerical_columns = [\n",
    "         \"tx_amount\",\n",
    "         \"is_weekend\",\n",
    "         \"is_night\",\n",
    "         \"customer_id_nb_tx_1day_window\",\n",
    "         \"customer_id_avg_amount_1day_window\",\n",
    "         \"customer_id_nb_tx_7day_window\",\n",
    "         \"customer_id_avg_amount_7day_window\",\n",
    "         \"customer_id_nb_tx_30day_window\",\n",
    "         \"customer_id_avg_amount_30day_window\",\n",
    "         \"terminal_id_nb_tx_1day_window\",\n",
    "         \"terminal_id_risk_1day_window\",\n",
    "         \"terminal_id_nb_tx_7day_window\",\n",
    "         \"terminal_id_risk_7day_window\",\n",
    "         \"terminal_id_nb_tx_30day_window\",\n",
    "         \"terminal_id_risk_30day_window\"\n",
    "    ]\n",
    "    \n",
    "    #for column in numerical_columns:\n",
    "        #vector_assembler = VectorAssembler(inputCols=[column], outputCol=f\"{column}_vec\")\n",
    "        #scaler = StandardScaler(inputCol=vector_assembler.getOutputCol(), outputCol=f\"{column}_scaled\", withStd=True, withMean=True)\n",
    "        #stages += [vector_assembler] #, scaler]\n",
    "\n",
    "    # Combine all the transformed features into a single \"features\" column\n",
    "    #assembler_input = [f\"{column}_ohe\" for column in categorical_columns] + [f\"{column}_scaled\" for column in numerical_columns] \n",
    "    assembler_input = [column for column in numerical_columns] \n",
    "    vector_assembler = VectorAssembler(inputCols=assembler_input, outputCol=\"features\")\n",
    "    stages += [vector_assembler]\n",
    "    \n",
    "    # Add model\n",
    "    #classification = RandomForestClassifier(featuresCol='features', labelCol='tx_fraud')\n",
    "    classification = LogisticRegression(featuresCol='features', labelCol='tx_fraud')\n",
    "    stages += [classification]\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "#     # Fit the pipeline to the transactions DataFrame\n",
    "#     pipeline_model = pipeline.fit(df)\n",
    "\n",
    "#     # Transform the data\n",
    "#     df_transformed = pipeline_model.transform(df)\n",
    "    \n",
    "#     #df_transformed.show(truncate=False)\n",
    "\n",
    "#     # Show the transformed data\n",
    "#     df_transformed = df_transformed.select(\"year\", \"month\", \"features\", \"tx_fraud\")\n",
    "\n",
    "#     df_transformed.show(truncate=False)\n",
    "    \n",
    "#     return df_transformed\n",
    "    return pipeline\n",
    "\n",
    "def calculate_accuracy(predictions):\n",
    "    predictions = predictions.withColumn(\n",
    "        \"fraudPrediction\",\n",
    "        when((predictions.tx_fraud==1) & (predictions.prediction==1), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    accurateFraud = predictions.groupBy(\"fraudPrediction\").count().where(predictions.fraudPrediction==1).head()[1]\n",
    "    totalFraud = predictions.groupBy(\"tx_fraud\").count().where(predictions.tx_fraud==1).head()[1]\n",
    "    accuracy = (accurateFraud/totalFraud)*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce686865-eec4-4e55-922a-666390799fac",
   "metadata": {},
   "source": [
    "# Train and log with mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df605ead-bb9b-4222-acce-f32613e5e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting new model / inference pipeline ...\n",
      "Scoring the model ...\n",
      "Logging metrics to MLflow run 3837f9ac5093472ebcc16699699354ad ...\n",
      "Model ROC-train: 0.8141328175899902\n",
      "Model ROC-test: 0.872197153901282\n",
      "+---------------+-------+\n",
      "|fraudPrediction|  count|\n",
      "+---------------+-------+\n",
      "|              1| 473090|\n",
      "|              0|8927455|\n",
      "+---------------+-------+\n",
      "\n",
      "FraudPredictionAccuracy: 74.98224859573712\n",
      "Exporting/logging model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'fraud_classifier' already exists. Creating a new version of this model...\n",
      "2023/06/19 16:33:21 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: fraud_classifier, version 14\n",
      "Created version '14' of model 'fraud_classifier'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = (\n",
    "        pyspark.sql.SparkSession.builder\n",
    "            #.config('spark.executor.instances', 8)\n",
    "            .config(\"spark.executor.cores\", 4)\n",
    "            .appName(\"fraud_data_train\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    spark.conf.set('spark.sql.repl.eagerEval.enabled', True)  # to pretty print pyspark.DataFrame in jupyter\n",
    "    \n",
    "    df = spark.read.parquet(\"/user/transformed_full/\")\n",
    "    df_train = df.filter(col('ts').between(\"2019-09-21\", \"2019-10-13\"))\n",
    "    df_test = df.filter(col('ts').between(\"2019-10-21\", \"2019-10-27\"))\n",
    "\n",
    "    #df.show()\n",
    "    #print(df.dtypes)\n",
    "    # Prepare MLFlow experiment for logging\n",
    "    client = MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(\"Fraud_Data\")\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "    run_name = 'Fraud_data_pipeline' + ' ' + str(datetime.now())\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "        inf_pipeline = build_train_pipeline()\n",
    "\n",
    "        print(\"Fitting new model / inference pipeline ...\")\n",
    "        model = inf_pipeline.fit(df_train)\n",
    "\n",
    "        print(\"Scoring the model ...\")\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol='tx_fraud', rawPredictionCol='prediction')\n",
    "\n",
    "        predictions_train = model.transform(df_train)\n",
    "        predictions_train.head()\n",
    "        areaUnderROC_train = evaluator.evaluate(predictions_train)\n",
    "\n",
    "        predictions_test = model.transform(df_test)\n",
    "        areaUnderROC_test = evaluator.evaluate(predictions_test)\n",
    "\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        print(f\"Logging metrics to MLflow run {run_id} ...\")\n",
    "        mlflow.log_metric(\"ROC-train\", areaUnderROC_train)\n",
    "        print(f\"Model ROC-train: {areaUnderROC_train}\")\n",
    "        mlflow.log_metric(\"ROC-test\", areaUnderROC_test)\n",
    "        print(f\"Model ROC-test: {areaUnderROC_test}\")\n",
    "\n",
    "        #print(\"Saving model ...\")\n",
    "        #mlflow.spark.save_model(model, 'fraud_classifier')\n",
    "        predictions_test = predictions_test.withColumn(\n",
    "            \"fraudPrediction\", when((predictions_test.tx_fraud==1) & (predictions_test.prediction==1), 1).otherwise(0))\n",
    "        predictions_test.groupBy(\"fraudPrediction\").count().show()\n",
    "\n",
    "        accurateFraud = predictions_test.groupBy(\"fraudPrediction\").count().where(predictions_test.fraudPrediction==1).head()[1]\n",
    "        totalFraud = predictions_test.groupBy(\"tx_fraud\").count().where(predictions_test.tx_fraud==1).head()[1]\n",
    "        FraudPredictionAccuracy = (accurateFraud/totalFraud)*100\n",
    "        print(\"FraudPredictionAccuracy:\", FraudPredictionAccuracy)\n",
    "\n",
    "        print(\"Exporting/logging model ...\")\n",
    "        mlflow.spark.log_model(model, 'fraud_classifier', registered_model_name='fraud_classifier')\n",
    "        print(\"Done\")\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b97351-356c-46e2-9252-e9170510a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_files = get_new_files_list()\n",
    "# new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed5a7ac-4074-4c48-8a13-3a7e4fe78c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = read_files(new_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e04e0b-967a-416f-b6c4-09098e640fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61641f63-9790-4d0d-9acf-1c4df5887923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d5d465-715c-4d9d-83d1-880573cf7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare MLFlow experiment for logging\n",
    "# client = MlflowClient()\n",
    "# experiment = client.get_experiment_by_name(\"Fraud_Data\")\n",
    "# experiment_id = experiment.experiment_id\n",
    "\n",
    "# run_name = 'Fraud_data_pipeline' + ' ' + str(datetime.now())\n",
    "\n",
    "# with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "#     inf_pipeline = build_train_pipeline()\n",
    "    \n",
    "#     # Balance classes\n",
    "#     majority_class = df.filter(df['tx_fraud'] == 0)\n",
    "#     minority_class = df.filter(df['tx_fraud'] == 1)\n",
    "#     #sampled_majority_class = majority_class.sample(False, 1/16, seed=42)  # set the fraction based on your needs\n",
    "#     balanced_df = majority_class.limit(minority_class.count()).union(minority_class)\n",
    "#     balanced_df = balanced_df.sample(withReplacement=False, fraction=1.0, seed=42)\n",
    "#     print(\"Number of items of each class after rebalancing:\")\n",
    "#     balanced_df.groupBy('tx_fraud').count().show()\n",
    "    \n",
    "#     train, test = balanced_df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "#     print(\"Fitting new model / inference pipeline ...\")\n",
    "#     model = inf_pipeline.fit(train)\n",
    "\n",
    "#     print(\"Scoring the model ...\")\n",
    "#     evaluator = BinaryClassificationEvaluator(labelCol='tx_fraud', rawPredictionCol='prediction')\n",
    "    \n",
    "#     predictions_train = model.transform(train)\n",
    "#     predictions_train.head()\n",
    "#     areaUnderROC_train = evaluator.evaluate(predictions_train)\n",
    "    \n",
    "#     predictions_test = model.transform(test)\n",
    "#     areaUnderROC_test = evaluator.evaluate(predictions_test)\n",
    "\n",
    "#     run_id = mlflow.active_run().info.run_id\n",
    "#     print(f\"Logging metrics to MLflow run {run_id} ...\")\n",
    "#     mlflow.log_metric(\"ROC-train\", areaUnderROC_train)\n",
    "#     print(f\"Model ROC-train: {areaUnderROC_train}\")\n",
    "#     mlflow.log_metric(\"ROC-test\", areaUnderROC_test)\n",
    "#     print(f\"Model ROC-test: {areaUnderROC_test}\")\n",
    "\n",
    "#     #print(\"Saving model ...\")\n",
    "#     #mlflow.spark.save_model(model, 'fraud_classifier')\n",
    "#     predictions_test = predictions_test.withColumn(\n",
    "#         \"fraudPrediction\", when((predictions_test.tx_fraud==1) & (predictions_test.prediction==1), 1).otherwise(0))\n",
    "#     predictions_test.groupBy(\"fraudPrediction\").count().show()\n",
    "    \n",
    "#     accurateFraud = predictions_test.groupBy(\"fraudPrediction\").count().where(predictions_test.fraudPrediction==1).head()[1]\n",
    "#     totalFraud = predictions_test.groupBy(\"tx_fraud\").count().where(predictions_test.tx_fraud==1).head()[1]\n",
    "#     FraudPredictionAccuracy = (accurateFraud/totalFraud)*100\n",
    "#     print(\"FraudPredictionAccuracy:\", FraudPredictionAccuracy)\n",
    "\n",
    "#     print(\"Exporting/logging model ...\")\n",
    "#     mlflow.spark.log_model(model, 'fraud_classifier', registered_model_name='fraud_classifier')\n",
    "#     print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574f104f-f353-455d-81c1-17d065cd6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913c627-f6ae-4045-ab5e-d6258e7f41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
