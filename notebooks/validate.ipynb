{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd90990e-aae2-45bf-9cf0-3b02a9cfdf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e79d6a-75f4-498d-b693-5f7f20a878f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "import pyspark\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import hour, minute, second, year, month, dayofmonth, dayofweek, count, to_timestamp, when, isnan\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from pyspark.sql.functions import countDistinct, udf\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd25ff67-c05a-4dd9-a5fb-5d054145aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_cmd(args_list):\n",
    "#     \"\"\"\n",
    "#     run linux commands\n",
    "#     \"\"\"\n",
    "#     # import subprocess\n",
    "#     print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "#     proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#     s_output, s_err = proc.communicate()\n",
    "#     s_return =  proc.returncode\n",
    "#     return s_return, s_output, s_err \n",
    "\n",
    "# def get_files_list():        \n",
    "#     # Get file list from hdfs\n",
    "#     (ret, out, err) = run_cmd(['hdfs', 'dfs', '-ls', '-C', '/user/fraud-data'])\n",
    "#     hdfs_files = [line for line in out.decode().split('\\n') if len(line)]\n",
    "#     hdfs_files.sort()\n",
    "#     return hdfs_files\n",
    "\n",
    "\n",
    "# def read_files(file_list):\n",
    "#     # Define the schema for the DataFrame\n",
    "#     schema = StructType([\n",
    "#         StructField(\"transaction_id\", IntegerType(), True),\n",
    "#         StructField(\"tx_datetime\", StringType(), True),\n",
    "#         StructField(\"customer_id\", IntegerType(), True),\n",
    "#         StructField(\"terminal_id\", IntegerType(), True),\n",
    "#         StructField(\"tx_amount\", DoubleType(), True),\n",
    "#         StructField(\"tx_time_seconds\", IntegerType(), True),\n",
    "#         StructField(\"tx_time_days\", IntegerType(), True),\n",
    "#         StructField(\"tx_fraud\", IntegerType(), True),\n",
    "#         StructField(\"tx_fraud_scenario\", IntegerType(), True)\n",
    "#     ])\n",
    "\n",
    "#     # Load the CSV file into a DataFrame\n",
    "#     df = (spark.read\n",
    "#         .format(\"csv\")\n",
    "#         .schema(schema)\n",
    "#         .option(\"header\", False)\n",
    "#         .option(\"sep\", ',')\n",
    "#         .option(\"comment\", '#')\n",
    "#         .load(file_list)\n",
    "#     )\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # Define a UDF to handle the special case\n",
    "# def convert_timestamp(s):\n",
    "#     if s[11:13] == '24':\n",
    "#         return s[:11] + '00' + s[13:]\n",
    "#     return s\n",
    "\n",
    "\n",
    "# def preprocess(df):\n",
    "#     # Convert the tx_datetime column to a timestamp type\n",
    "#     df = df.limit(df.count() // 10)\n",
    "#     convert_timestamp_udf = udf(convert_timestamp)\n",
    "#     df = df.withColumn(\"tx_datetime\", convert_timestamp_udf(df[\"tx_datetime\"]))\n",
    "#     df = df.withColumn(\"ts\", to_timestamp(df[\"tx_datetime\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    \n",
    "#     df = df.fillna({'terminal_id': 0})\n",
    "    \n",
    "#     # Extract new features from the tx_datetime column\n",
    "#     df = df.withColumn(\"is_weekend\", dayofweek(\"ts\").isin([1,7]).cast(\"int\"))\n",
    "#     #df = df.withColumn(\"year\", year(df[\"ts\"]))\n",
    "#     #df = df.withColumn(\"month\", month(df[\"ts\"]))\n",
    "#     df = df.withColumn(\"day_of_month\", dayofmonth(df[\"ts\"]))\n",
    "#     df = df.withColumn(\"day_of_week\", dayofweek(df[\"ts\"]))\n",
    "#     df = df.withColumn(\"hour\", hour(df[\"ts\"]))\n",
    "#     df = df.withColumn(\"minute\", minute(df[\"ts\"]))\n",
    "#     df = df.withColumn(\"second\", second(df[\"ts\"]))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "def calculate_accuracy(predictions):\n",
    "    predictions = predictions.withColumn(\n",
    "        \"fraudPrediction\",\n",
    "        when((predictions.tx_fraud==1) & (predictions.prediction==1), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    accurateFraud = predictions.groupBy(\"fraudPrediction\").count().where(predictions.fraudPrediction==1).head()[1]\n",
    "    totalFraud = predictions.groupBy(\"tx_fraud\").count().where(predictions.tx_fraud==1).head()[1]\n",
    "    accuracy = (accurateFraud/totalFraud)*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c69ad8a-db11-4b5d-8be9-f25cd610fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/19 16:35:20 INFO mlflow.spark: 'models:/fraud_classifier/latest' resolved as 's3://mlops-hw/2/3837f9ac5093472ebcc16699699354ad/artifacts/fraud_classifier'\n",
      "2023/06/19 16:35:21 INFO mlflow.spark: URI 'models:/fraud_classifier/latest/sparkml' does not point to the current DFS.\n",
      "2023/06/19 16:35:21 INFO mlflow.spark: File 'models:/fraud_classifier/latest/sparkml' not found on DFS. Will attempt to upload the file.\n",
      "2023/06/19 16:35:21 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/da6569a6-20ff-44a0-842c-3e61dc67492a\n",
      "2023/06/19 16:35:25 INFO mlflow.spark: 'models:/fraud_classifier/Staging' resolved as 's3://mlops-hw/2/349ee05a17c847ac829636098a4a0a9f/artifacts/fraud_classifier'\n",
      "2023/06/19 16:35:26 INFO mlflow.spark: URI 'models:/fraud_classifier/Staging/sparkml' does not point to the current DFS.\n",
      "2023/06/19 16:35:26 INFO mlflow.spark: File 'models:/fraud_classifier/Staging/sparkml' not found on DFS. Will attempt to upload the file.\n",
      "2023/06/19 16:35:27 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/759bd767-cd1b-4d90-bea0-8b56a0ab30d3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging metrics to MLflow run 4f518306d7864a0d8038b571b6550089 ...\n",
      "Model ROC-latest: 0.8913836677432935\n",
      "Model Acc-latest: 78.73607373858214\n",
      "Model ROC-staging: 0.889044246825055\n",
      "Model Acc-staging: 78.23287480602342\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = (\n",
    "        pyspark.sql.SparkSession.builder\n",
    "            #.config('spark.executor.instances', 8)\n",
    "            .config(\"spark.executor.cores\", 4)\n",
    "            .appName(\"fraud_data_validate\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    df = spark.read.parquet(\"/user/transformed_full/\")\n",
    "    df_validate = df.filter(col('ts').between(\"2019-10-28\", \"2019-11-05\"))\n",
    "    \n",
    "    # Prepare MLFlow experiment for logging\n",
    "    client = MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(\"Fraud_Data_Validate\")\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "    run_name = 'Fraud_data_validate' + ' ' + str(datetime.now())\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "        # Load models\n",
    "        model_latest = mlflow.spark.load_model(model_uri=f\"models:/fraud_classifier/latest\")\n",
    "        model_staging = mlflow.spark.load_model(model_uri=f\"models:/fraud_classifier/Staging\")\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol='tx_fraud', rawPredictionCol='prediction')\n",
    "\n",
    "        # Perform inference via model.transform()\n",
    "        predictions_latest = model_latest.transform(df_validate)\n",
    "        areaUnderROC_latest = evaluator.evaluate(predictions_latest)\n",
    "        accuracy_latest = calculate_accuracy(predictions_latest)\n",
    "        predictions_staging = model_staging.transform(df_validate)\n",
    "        areaUnderROC_staging = evaluator.evaluate(predictions_staging)\n",
    "        accuracy_staging = calculate_accuracy(predictions_staging)\n",
    "\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        print(f\"Logging metrics to MLflow run {run_id} ...\")\n",
    "        mlflow.log_metric(\"ROC-latest\", areaUnderROC_latest)\n",
    "        mlflow.log_metric(\"Acc-latest\", accuracy_latest)\n",
    "        print(f\"Model ROC-latest: {areaUnderROC_latest}\")\n",
    "        print(f\"Model Acc-latest: {accuracy_latest}\")\n",
    "        \n",
    "        mlflow.log_metric(\"ROC-staging\", areaUnderROC_staging)\n",
    "        mlflow.log_metric(\"Acc-staging\", accuracy_staging)\n",
    "        print(f\"Model ROC-staging: {areaUnderROC_staging}\")\n",
    "        print(f\"Model Acc-staging: {accuracy_staging}\")\n",
    "        \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab77bb20-5c52-4056-8f27-8d000219fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main\n",
    "\n",
    "# spark = (\n",
    "#     pyspark.sql.SparkSession.builder\n",
    "#         .appName(\"fraud_data_validate\")\n",
    "#         .getOrCreate()\n",
    "# )\n",
    "# spark.conf.set('spark.sql.repl.eagerEval.enabled', True)  # to pretty print pyspark.DataFrame in jupyter\n",
    "\n",
    "# # Read available files\n",
    "# new_files = get_files_list()\n",
    "# print(new_files)\n",
    "\n",
    "# df = read_files(new_files[1])\n",
    "# df = preprocess(df)\n",
    "\n",
    "# # Prepare MLFlow experiment for logging\n",
    "# client = MlflowClient()\n",
    "# experiment = client.get_experiment_by_name(\"Fraud_Data_Validate\")\n",
    "# experiment_id = experiment.experiment_id\n",
    "\n",
    "# run_name = 'Fraud_data_validate' + ' ' + str(datetime.now())\n",
    "\n",
    "# with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "#     test = df.sample(0.1)\n",
    "\n",
    "#     # Load models\n",
    "#     model_latest = mlflow.spark.load_model(model_uri=f\"models:/fraud_classifier/latest\")\n",
    "#     model_staging = mlflow.spark.load_model(model_uri=f\"models:/fraud_classifier/Staging\")\n",
    "#     evaluator = BinaryClassificationEvaluator(labelCol='tx_fraud', rawPredictionCol='prediction')\n",
    "\n",
    "#     # Perform inference via model.transform()\n",
    "#     predictions_latest = model_latest.transform(test)\n",
    "#     areaUnderROC_latest = evaluator.evaluate(predictions_latest)\n",
    "#     predictions_staging = model_staging.transform(test)\n",
    "#     areaUnderROC_staging = evaluator.evaluate(predictions_staging)\n",
    "\n",
    "#     run_id = mlflow.active_run().info.run_id\n",
    "#     print(f\"Logging metrics to MLflow run {run_id} ...\")\n",
    "#     mlflow.log_metric(\"ROC-latest\", areaUnderROC_latest)\n",
    "#     print(f\"Model ROC-latest: {areaUnderROC_latest}\")\n",
    "#     mlflow.log_metric(\"ROC-staging\", areaUnderROC_staging)\n",
    "#     print(f\"Model ROC-staging: {areaUnderROC_staging}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44deb957-7c6c-47db-8c86-50cb7559aae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
