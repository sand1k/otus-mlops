{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941c0001-b75b-45dc-8d02-881dcd45ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6503362c-e962-47c4-896f-917c13d7e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import (\n",
    "    hour,\n",
    "    dayofweek,\n",
    "    year,\n",
    "    month,\n",
    "    dayofmonth,\n",
    "    count,\n",
    "    to_timestamp,\n",
    "    when,\n",
    "    isnan,\n",
    "    col,\n",
    "    udf\n",
    ")\n",
    "from pyspark.sql.functions import sum, avg, count, col, lit\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cce5af-18aa-43a3-87d8-09657554475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cmd(args_list):\n",
    "    \"\"\"\n",
    "    run linux commands\n",
    "    \"\"\"\n",
    "    # import subprocess\n",
    "    print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    s_output, s_err = proc.communicate()\n",
    "    s_return =  proc.returncode\n",
    "    return s_return, s_output, s_err\n",
    "\n",
    "def get_files_list():        \n",
    "    # Get file list from hdfs\n",
    "    (ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', '-C', '/user/fraud-data'])\n",
    "    hdfs_files = [line for line in out.decode().split('\\n') if len(line)]\n",
    "    hdfs_files.sort()\n",
    "    return hdfs_files\n",
    "\n",
    "\n",
    "def read_files(file_list):\n",
    "    # Define the schema for the DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), True),\n",
    "        StructField(\"tx_datetime\", StringType(), True),\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"terminal_id\", IntegerType(), True),\n",
    "        StructField(\"tx_amount\", DoubleType(), True),\n",
    "        StructField(\"tx_time_seconds\", IntegerType(), True),\n",
    "        StructField(\"tx_time_days\", IntegerType(), True),\n",
    "        StructField(\"tx_fraud\", IntegerType(), True),\n",
    "        StructField(\"tx_fraud_scenario\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = (spark.read\n",
    "        .format(\"csv\")\n",
    "        .schema(schema)\n",
    "        .option(\"header\", False)\n",
    "        .option(\"sep\", ',')\n",
    "        .option(\"comment\", '#')\n",
    "        .load(file_list)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define a UDF to handle the special case\n",
    "def convert_timestamp(s):\n",
    "    if s[11:13] == '24':\n",
    "        return s[:11] + '00' + s[13:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def is_night(tx_hour):\n",
    "    is_night = tx_hour <= 6\n",
    "    return int(is_night)\n",
    "\n",
    "\n",
    "# Define function\n",
    "def get_customer_spending_behaviour_features(transactions_df, windows_size_in_days=[1,7,30]):\n",
    "    # For each window size\n",
    "    for window_size in windows_size_in_days:\n",
    "        # Define the window\n",
    "        window = Window \\\n",
    "            .partitionBy(\"customer_id\") \\\n",
    "            .orderBy(col(\"ts\").cast(\"long\")) \\\n",
    "            .rangeBetween(-window_size * 86400, 0)  # 86400 is number of seconds in a day\n",
    "\n",
    "        # Compute the sum of the transaction amounts, the number of transactions and the average transaction amount for the given window size\n",
    "        transactions_df = transactions_df \\\n",
    "            .withColumn('nb_tx_window', count(\"tx_amount\").over(window)) \\\n",
    "            .withColumn('avg_amount_tx_window', avg(\"tx_amount\").over(window))\n",
    "\n",
    "        # Rename the columns\n",
    "        transactions_df = transactions_df \\\n",
    "            .withColumnRenamed('nb_tx_window', 'customer_id_nb_tx_' + str(window_size) + 'day_window') \\\n",
    "            .withColumnRenamed('avg_amount_tx_window', 'customer_id_avg_amount_' + str(window_size) + 'day_window')\n",
    "\n",
    "    # Return the dataframe with the new features\n",
    "    return transactions_df\n",
    "\n",
    "\n",
    "def get_count_risk_rolling_window(transactions_df, delay_period=7, windows_size_in_days=[1,7,30], precision_loss=1):\n",
    "    # Define the delay window\n",
    "    delay_window = Window \\\n",
    "        .partitionBy(\"terminal_id\") \\\n",
    "        .orderBy(col(\"truncated_ts\").cast(\"long\")) \\\n",
    "        .rangeBetween(-delay_period * 86400 // precision_loss, 0)  \n",
    "\n",
    "    # Compute the number of frauds and transactions for the delay window\n",
    "    transactions_df = transactions_df \\\n",
    "        .withColumn('nb_fraud_delay', sum(\"tx_fraud\").over(delay_window)) \\\n",
    "        .withColumn('nb_tx_delay', count(\"tx_fraud\").over(delay_window)) \n",
    "\n",
    "    for window_size in windows_size_in_days:\n",
    "        # Define the window including the delay period and the window size\n",
    "        delay_window_size = Window \\\n",
    "            .partitionBy(\"terminal_id\") \\\n",
    "            .orderBy(col(\"truncated_ts\").cast(\"long\")) \\\n",
    "            .rangeBetween(-(delay_period + window_size) * 86400 // precision_loss, 0)\n",
    "\n",
    "        # Compute the number of frauds and transactions for the delay window size\n",
    "        transactions_df = transactions_df \\\n",
    "            .withColumn('nb_fraud_delay_window', sum(\"tx_fraud\").over(delay_window_size)) \\\n",
    "            .withColumn('nb_tx_delay_window', count(\"tx_fraud\").over(delay_window_size)) \n",
    "\n",
    "        # Compute the number of frauds and transactions for the window size\n",
    "        transactions_df = transactions_df \\\n",
    "            .withColumn('nb_fraud_window', col('nb_fraud_delay_window') - col('nb_fraud_delay')) \\\n",
    "            .withColumn('nb_tx_window', col('nb_tx_delay_window') - col('nb_tx_delay')) \n",
    "\n",
    "        # Compute the risk for the window size\n",
    "        transactions_df = transactions_df \\\n",
    "            .withColumn('risk_window', when(col('nb_tx_window') > 0, col('nb_fraud_window') / col('nb_tx_window')).otherwise(0))\n",
    "\n",
    "        # Rename the columns\n",
    "        transactions_df = transactions_df \\\n",
    "            .withColumnRenamed('nb_tx_window', 'terminal_id_nb_tx_' + str(window_size) + 'day_window') \\\n",
    "            .withColumnRenamed('risk_window', 'terminal_id_risk_' + str(window_size) + 'day_window') \n",
    "\n",
    "    return transactions_df\n",
    "\n",
    "        \n",
    "def preprocess(df_orig):\n",
    "    df = df_orig.na.drop()\n",
    "    dropped_cnt = df_orig.count() - df.count()\n",
    "    print(f\"Dropped {dropped_cnt} rows with null values.\")\n",
    "    \n",
    "    convert_timestamp_udf = udf(convert_timestamp)\n",
    "    df = df.withColumn(\"tx_datetime\", convert_timestamp_udf(df[\"tx_datetime\"]))\n",
    "    df = df.withColumn(\"ts\", to_timestamp(df[\"tx_datetime\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    \n",
    "    # Extract new features from the tx_datetime column\n",
    "    df = df.withColumn(\"year\", year(df[\"ts\"]))\n",
    "    df = df.withColumn(\"month\", month(df[\"ts\"]))\n",
    "    df = df.withColumn(\"day\", dayofmonth(df[\"ts\"]))\n",
    "    df = df.withColumn(\"is_weekend\", dayofweek(\"ts\").isin([1,7]).cast(\"int\"))\n",
    "    is_night_udf = udf(is_night, IntegerType())\n",
    "    df = df.withColumn(\"is_night\", is_night_udf(hour(\"ts\")))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff043e0f-803b-4f2c-9122-2d92ca561f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -ls -C /user/fraud-data\n",
      "['/user/fraud-data/2019-08-22.txt', '/user/fraud-data/2019-09-21.txt', '/user/fraud-data/2019-10-21.txt']\n",
      "Row count: 140977436\n",
      "Check null values:\n",
      "+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|transaction_id|tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|             0|          0|          0|       1115|        0|              0|           0|       0|                0|\n",
      "+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "\n",
      "Dropped 1115 rows with null values.\n",
      "Writing data to HDFS...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #\n",
    "    # Part I\n",
    "    #\n",
    "    spark = (\n",
    "        pyspark.sql.SparkSession.builder\n",
    "            #.config('spark.executor.instances', 8)\n",
    "            .config(\"spark.executor.cores\", 4)\n",
    "            .appName(\"fraud_data_preproc\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    spark.conf.set('spark.sql.repl.eagerEval.enabled', True)  # to pretty print pyspark.DataFrame in jupyter\n",
    "    \n",
    "    new_files = get_files_list()\n",
    "    print(new_files)\n",
    "    \n",
    "    df = read_files(new_files)\n",
    "    \n",
    "    # Calculate rows number\n",
    "    row_count = df.count()\n",
    "    print(f\"Row count: {row_count}\")\n",
    "    \n",
    "    # Find null values\n",
    "    print(\"Check null values:\")\n",
    "    null_vals = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    null_vals.show()\n",
    "    \n",
    "    df = preprocess(df)\n",
    "    \n",
    "    # Order the dataframe\n",
    "    df = df.orderBy('ts')\n",
    "    df = get_customer_spending_behaviour_features(df, windows_size_in_days=[1, 7, 30])\n",
    "    \n",
    "    # Decrease the precision but calculate faster\n",
    "    precision_loss = 3600\n",
    "    df = df.withColumn(\"truncated_ts\", (col(\"ts\").cast(\"long\") / precision_loss).cast(\"long\"))\n",
    "    df = get_count_risk_rolling_window(\n",
    "        df,\n",
    "        delay_period=7,\n",
    "        windows_size_in_days=[1, 7, 30],\n",
    "        precision_loss=precision_loss)\n",
    "    \n",
    "    # Save the transformed data as Parquet\n",
    "    print(\"Writing data to HDFS...\")\n",
    "    output_path = \"/user/transformed_full/\"\n",
    "    (df\n",
    "         .write\n",
    "         .partitionBy(\"year\", \"month\", \"day\")\n",
    "         .parquet(output_path, mode=\"overwrite\")\n",
    "    )\n",
    "    print(\"Data saved.\")\n",
    "    spark.stop()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
